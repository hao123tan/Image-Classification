{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install q keras==2.3.1\n",
    "!pip install q tensorflow-gpu==2.1.0\n",
    "!pip install q tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessData(X_Train, Y_Train, X_Test, Y_Test, nums_class):\n",
    "    #Normalize picture\n",
    "    X_Train = X_Train.astype('float32')/255.\n",
    "    X_Test = X_Test.astype('float32')/255.\n",
    "    #split training data into training and validation set\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_Train, Y_Train, test_size=0.2, random_state=7, shuffle= True)\n",
    "    Y_train = keras.utils.to_categorical(Y_train)\n",
    "    Y_valid = keras.utils.to_categorical(Y_valid)\n",
    "    Y_Test = keras.utils.to_categorical(Y_Test)\n",
    "    return ((X_train, Y_train), (X_valid, Y_valid), (X_Test, Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aug_Data(X_train, Y_train, X_valid, Y_valid, batch_size):\n",
    "    train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    )\n",
    "    validation_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
    "    train_generator = train_datagen.flow(X_train, Y_train, batch_size=batch_size)\n",
    "    valid_generator = validation_datagen.flow(X_valid, Y_valid, batch_size=batch_size)\n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel(filter_number, layer_number, dropout, optimization):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=filter_number, \n",
    "                                  kernel_size=(3,3), \n",
    "                                  padding='same', \n",
    "                                  activation='relu',\n",
    "                                  input_shape=[width, height, channels]))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(dropout)),\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    for i in range(layer_number):\n",
    "        model.add(keras.layers.Conv2D(filters=filter_number*(2**(i+1)), \n",
    "                                  kernel_size=(3,3), \n",
    "                                  padding='same', \n",
    "                                  activation='relu',\n",
    "                                  input_shape=[width, height, channels]))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        keras.layers.Dropout(dropout)\n",
    "        model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(256))\n",
    "    model.add(keras.layers.ReLU())\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimization, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Process(train_generator, valid_generator, MLName):\n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    cp = keras.callbacks.ModelCheckpoint(filepath = MLName,save_best_only = True,verbose=1)\n",
    "    lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_delta=0.0001)\n",
    "    callbacks = [earlystop, cp, lr]\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch = len(train_generator) // batch_size,\n",
    "                        epochs = epochs,\n",
    "                        callbacks = callbacks,\n",
    "                        validation_data = valid_generator,\n",
    "                        validation_steps = len(valid_generator) // batch_size)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_map(model, img):\n",
    "    layer_names = [layer.name for layer in model.layers]\n",
    "    layer_outputs = [layer.output for layer in model.layers]\n",
    "    new_model=keras.models.Model(inputs=model.inputs,outputs=layer_outputs)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    feature_maps=new_model.predict(img)\n",
    "    \n",
    "    for layer_name, feature_map in zip(layer_names, feature_maps):\n",
    "        if len(feature_map.shape) == 4:\n",
    "            n_features = feature_map.shape[-1]\n",
    "            size       = feature_map.shape[ 1]\n",
    "            display_grid = np.zeros((size, size * n_features))\n",
    "\n",
    "            for i in range(n_features):\n",
    "                x  = feature_map[0, :, :, i]\n",
    "                x -= x.mean()\n",
    "                x /= x.std ()\n",
    "                x *=  64\n",
    "                x += 128\n",
    "                x  = np.clip(x, 0, 255).astype('uint8')\n",
    "                display_grid[:, i * size : (i + 1) * size] = x\n",
    "            scale = 20. / n_features\n",
    "            plt.figure( figsize=(scale * n_features, scale) )\n",
    "            plt.title ( layer_name )\n",
    "            plt.grid  ( False )\n",
    "            plt.imshow( display_grid, aspect='auto', cmap='viridis' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = keras.datasets.cifar100.load_data()\n",
    "(x_train_all, y_train_all), (x_test, y_test) = cifar100\n",
    "(X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test) = PreprocessData(x_train_all, y_train_all, x_test, y_test, num_classes)\n",
    "train_generator, valid_generator = Aug_Data(X_train, Y_train, X_valid, Y_valid, batch_size)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BuildModel(32, 2, 0.25, 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = training_Process(train_generator, valid_generator, 'cifar100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./cifar100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize_map(model, X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_learning_curves(history, label, epochs, min_value, max_value, title):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.title (title)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "plots_learning_curves(history, 'acc', epochs, 0, 1, 'cifar100')\n",
    "plots_learning_curves(history, 'loss', epochs, 0, 10, 'cifar100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 and Cifar100 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = keras.datasets.cifar10.load_data()\n",
    "(x_train_all, y_train_all), (x_test, y_test) = cifar10\n",
    "(X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test) = PreprocessData(x_train_all, y_train_all, x_test, y_test, num_classes)\n",
    "train_generator, valid_generator = Aug_Data(X_train, Y_train, X_valid, Y_valid, batch_size)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BuildModel(64, 3, 0.25, 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = training_Process(train_generator, valid_generator, 'best-cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./cifar10-1.h5')\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_labels = ['dataset', 'layers', 'filter_number', 'dropout', 'optimizer', 'test_accuracy']\n",
    "value = [['cifar10', 2, 32, 0.25, 'adam', 0.6465], \n",
    "         ['cifar10', 2, 64, 0.25, 'adam', 0.5972], \n",
    "         ['cifar10', 3, 64, 0.25, 'adam', 0.6522],\n",
    "         ['cifar10', 3, 32, 0.35, 'adam', 0.6211],\n",
    "         ['cifar10', 2, 32, 0.25, 'SGD', 0.5799],\n",
    "         ['cifar100', 2, 32, 0.25, 'adam', 0.2279], \n",
    "         ['cifar100', 3, 32, 0.25, 'adam', 0.2431], \n",
    "         ['cifar100', 3, 64, 0.25, 'adam', 0.2128],\n",
    "         ['cifar100', 3, 32, 0.35, 'adam', 0.2272],\n",
    "         ['cifar100', 3, 32, 0.25, 'SGD', 0.1880],]\n",
    "pd.DataFrame(data=value, columns=parameter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
